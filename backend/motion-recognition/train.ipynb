{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3118, 30, 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [\n",
    "    'pet',\n",
    "    'heart',\n",
    "    'hi'\n",
    "]\n",
    "\n",
    "data = np.concatenate([\n",
    "    np.load('dataset/seq_pet_1643785507.npy'),\n",
    "    np.load('dataset/seq_heart_1643785507.npy'),\n",
    "    np.load('dataset/seq_hi_1643785507.npy')\n",
    "], axis=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3118, 30, 99)\n",
      "(3118,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3118, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2806, 30, 99) (2806, 3)\n",
      "(312, 30, 99) (312, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64)                41984     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 44,163\n",
      "Trainable params: 44,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 13.1667 - acc: 0.5930\n",
      "Epoch 00001: val_acc improved from -inf to 0.74359, saving model to models\\model2_1.0.h5\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 13.1281 - acc: 0.5944 - val_loss: 3.7664 - val_acc: 0.7436\n",
      "Epoch 2/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 9.0317 - acc: 0.7562\n",
      "Epoch 00002: val_acc did not improve from 0.74359\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 9.0317 - acc: 0.7562 - val_loss: 15.2884 - val_acc: 0.6571\n",
      "Epoch 3/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 7.6588 - acc: 0.7267\n",
      "Epoch 00003: val_acc improved from 0.74359 to 0.92308, saving model to models\\model2_1.0.h5\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 7.6588 - acc: 0.7267 - val_loss: 1.6521 - val_acc: 0.9231\n",
      "Epoch 4/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 4.8834 - acc: 0.7801\n",
      "Epoch 00004: val_acc did not improve from 0.92308\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 4.8834 - acc: 0.7801 - val_loss: 19.8356 - val_acc: 0.5032\n",
      "Epoch 5/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 24.8467 - acc: 0.6007\n",
      "Epoch 00005: val_acc did not improve from 0.92308\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 24.4674 - acc: 0.6037 - val_loss: 8.0746 - val_acc: 0.6955\n",
      "Epoch 6/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 22.8695 - acc: 0.6643\n",
      "Epoch 00006: val_acc did not improve from 0.92308\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 22.8695 - acc: 0.6643 - val_loss: 1.3993 - val_acc: 0.8974\n",
      "Epoch 7/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6089 - acc: 0.9252- ETA: 1s - \n",
      "Epoch 00007: val_acc improved from 0.92308 to 0.97756, saving model to models\\model2_1.0.h5\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 1.6089 - acc: 0.9252 - val_loss: 0.3296 - val_acc: 0.9776\n",
      "Epoch 8/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.9125 - acc: 0.9557\n",
      "Epoch 00008: val_acc did not improve from 0.97756\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.9105 - acc: 0.9555 - val_loss: 0.2084 - val_acc: 0.9776\n",
      "Epoch 9/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9752\n",
      "Epoch 00009: val_acc improved from 0.97756 to 0.98718, saving model to models\\model2_1.0.h5\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.2818 - acc: 0.9754 - val_loss: 0.1381 - val_acc: 0.9872\n",
      "Epoch 10/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5984 - acc: 0.9651\n",
      "Epoch 00010: val_acc did not improve from 0.98718\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.5984 - acc: 0.9651 - val_loss: 0.3769 - val_acc: 0.9776\n",
      "Epoch 11/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4713 - acc: 0.9736\n",
      "Epoch 00011: val_acc did not improve from 0.98718\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.4713 - acc: 0.9736 - val_loss: 0.1345 - val_acc: 0.9808\n",
      "Epoch 12/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.9738\n",
      "Epoch 00012: val_acc did not improve from 0.98718\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 0.3389 - acc: 0.9740 - val_loss: 0.4826 - val_acc: 0.9679\n",
      "Epoch 13/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1178 - acc: 0.9897\n",
      "Epoch 00013: val_acc improved from 0.98718 to 0.99359, saving model to models\\model2_1.0.h5\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.1178 - acc: 0.9897 - val_loss: 0.0377 - val_acc: 0.9936\n",
      "Epoch 14/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1212 - acc: 0.9904\n",
      "Epoch 00014: val_acc did not improve from 0.99359\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.1212 - acc: 0.9904 - val_loss: 0.2866 - val_acc: 0.9776\n",
      "Epoch 15/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0985 - acc: 0.9890\n",
      "Epoch 00015: val_acc did not improve from 0.99359\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0985 - acc: 0.9890 - val_loss: 0.3372 - val_acc: 0.9679\n",
      "Epoch 16/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.2505 - acc: 0.9840\n",
      "Epoch 00016: val_acc did not improve from 0.99359\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.2505 - acc: 0.9840 - val_loss: 0.3511 - val_acc: 0.9712\n",
      "Epoch 17/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.2843 - acc: 0.9804\n",
      "Epoch 00017: val_acc improved from 0.99359 to 1.00000, saving model to models\\model2_1.0.h5\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.2843 - acc: 0.9804 - val_loss: 6.3420e-04 - val_acc: 1.0000\n",
      "Epoch 18/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9878\n",
      "Epoch 00018: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.2009 - acc: 0.9879 - val_loss: 0.0227 - val_acc: 0.9936\n",
      "Epoch 19/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9971\n",
      "Epoch 00019: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 0.0260 - acc: 0.9971 - val_loss: 8.9929e-05 - val_acc: 1.0000\n",
      "Epoch 20/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9928\n",
      "Epoch 00020: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0525 - acc: 0.9929 - val_loss: 0.0303 - val_acc: 0.9968\n",
      "Epoch 21/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9978\n",
      "Epoch 00021: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0221 - acc: 0.9979 - val_loss: 0.0060 - val_acc: 0.9968\n",
      "Epoch 22/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0728 - acc: 0.9943\n",
      "Epoch 00022: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0728 - acc: 0.9943 - val_loss: 0.1147 - val_acc: 0.9904\n",
      "Epoch 23/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9869\n",
      "Epoch 00023: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.1406 - acc: 0.9865 - val_loss: 0.1043 - val_acc: 0.9936\n",
      "Epoch 24/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6998 - acc: 0.9633\n",
      "Epoch 00024: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.6864 - acc: 0.9640 - val_loss: 2.5663 - val_acc: 0.9231\n",
      "Epoch 25/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.6373 - acc: 0.9670\n",
      "Epoch 00025: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.6323 - acc: 0.9672 - val_loss: 0.0045 - val_acc: 0.9968\n",
      "Epoch 26/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.0748 - acc: 0.9562\n",
      "Epoch 00026: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.0748 - acc: 0.9562 - val_loss: 0.5526 - val_acc: 0.9712\n",
      "Epoch 27/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.9727\n",
      "Epoch 00027: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.5159 - acc: 0.9729 - val_loss: 0.1433 - val_acc: 0.9872\n",
      "Epoch 28/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1532 - acc: 0.9925\n",
      "Epoch 00028: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.1532 - acc: 0.9925 - val_loss: 0.9475 - val_acc: 0.9712\n",
      "Epoch 29/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.2146 - acc: 0.9608\n",
      "Epoch 00029: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.2090 - acc: 0.9608 - val_loss: 0.0835 - val_acc: 0.9936\n",
      "Epoch 30/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9896\n",
      "Epoch 00030: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0678 - acc: 0.9897 - val_loss: 0.0195 - val_acc: 0.9968\n",
      "Epoch 31/300\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.1364 - acc: 0.9912\n",
      "Epoch 00031: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.1517 - acc: 0.9904 - val_loss: 0.0103 - val_acc: 0.9968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9903\n",
      "Epoch 00032: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.1687 - acc: 0.9897 - val_loss: 2.5166 - val_acc: 0.8942\n",
      "Epoch 33/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.2770 - acc: 0.9840\n",
      "Epoch 00033: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.2770 - acc: 0.9840 - val_loss: 0.0968 - val_acc: 0.9904\n",
      "Epoch 34/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.2720 - acc: 0.9840\n",
      "Epoch 00034: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.2720 - acc: 0.9840 - val_loss: 0.2062 - val_acc: 0.9840\n",
      "Epoch 35/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 27.4911 - acc: 0.7217\n",
      "Epoch 00035: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 27.4911 - acc: 0.7217 - val_loss: 36.6756 - val_acc: 0.4391\n",
      "Epoch 36/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 6.8123 - acc: 0.6498\n",
      "Epoch 00036: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 6.7706 - acc: 0.6511 - val_loss: 1.4467 - val_acc: 0.7917\n",
      "Epoch 37/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.8544 - acc: 0.7608\n",
      "Epoch 00037: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 2.8788 - acc: 0.7587 - val_loss: 2.2795 - val_acc: 0.6827\n",
      "Epoch 38/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.2347 - acc: 0.8579\n",
      "Epoch 00038: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 1.2163 - acc: 0.8596 - val_loss: 1.4450 - val_acc: 0.8654\n",
      "Epoch 39/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.9926 - acc: 0.8835- ETA: 0s - loss: 0.9890 - acc: 0.8\n",
      "Epoch 00039: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 0.9926 - acc: 0.8835 - val_loss: 2.6319 - val_acc: 0.7596\n",
      "Epoch 40/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 2.0344 - acc: 0.8372\n",
      "Epoch 00040: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 2.0087 - acc: 0.8386 - val_loss: 0.9920 - val_acc: 0.9199\n",
      "Epoch 41/300\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 2.1373 - acc: 0.8456\n",
      "Epoch 00041: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 2.2327 - acc: 0.8411 - val_loss: 5.2668 - val_acc: 0.6346\n",
      "Epoch 42/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.8707 - acc: 0.9198\n",
      "Epoch 00042: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.8707 - acc: 0.9198 - val_loss: 1.2337 - val_acc: 0.9263\n",
      "Epoch 43/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 16.8935 - acc: 0.6628\n",
      "Epoch 00043: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 16.5945 - acc: 0.6604 - val_loss: 0.8802 - val_acc: 0.7564\n",
      "Epoch 44/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.6486 - acc: 0.8325\n",
      "Epoch 00044: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.6416 - acc: 0.8332 - val_loss: 0.4947 - val_acc: 0.8494\n",
      "Epoch 45/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3987 - acc: 0.8963\n",
      "Epoch 00045: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.3987 - acc: 0.8963 - val_loss: 0.2294 - val_acc: 0.9295\n",
      "Epoch 46/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.2491 - acc: 0.9298- ETA: 0s - loss: 0.28\n",
      "Epoch 00046: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.2491 - acc: 0.9298 - val_loss: 0.1421 - val_acc: 0.9519\n",
      "Epoch 47/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9320\n",
      "Epoch 00047: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.2632 - acc: 0.9319 - val_loss: 0.1874 - val_acc: 0.9327\n",
      "Epoch 48/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.2916 - acc: 0.9134\n",
      "Epoch 00048: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.2916 - acc: 0.9134 - val_loss: 0.3205 - val_acc: 0.8846\n",
      "Epoch 49/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8975\n",
      "Epoch 00049: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.3517 - acc: 0.8984 - val_loss: 0.2514 - val_acc: 0.9167\n",
      "Epoch 50/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9368\n",
      "Epoch 00050: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.2294 - acc: 0.9369 - val_loss: 0.1808 - val_acc: 0.9551\n",
      "Epoch 51/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9318\n",
      "Epoch 00051: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.2384 - acc: 0.9316 - val_loss: 0.1485 - val_acc: 0.9423\n",
      "Epoch 52/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9393\n",
      "Epoch 00052: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.2205 - acc: 0.9398 - val_loss: 0.1637 - val_acc: 0.9327\n",
      "Epoch 53/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9517\n",
      "Epoch 00053: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.1698 - acc: 0.9519 - val_loss: 0.1076 - val_acc: 0.9679\n",
      "Epoch 54/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 5.8946 - acc: 0.7942\n",
      "Epoch 00054: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 5.9464 - acc: 0.7944 - val_loss: 9.0341 - val_acc: 0.6667\n",
      "Epoch 55/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.6006 - acc: 0.6654\n",
      "Epoch 00055: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 2.6006 - acc: 0.6654 - val_loss: 0.3024 - val_acc: 0.9583\n",
      "Epoch 56/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1995 - acc: 0.9893\n",
      "Epoch 00056: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.1995 - acc: 0.9893 - val_loss: 0.1391 - val_acc: 0.9968\n",
      "Epoch 57/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9898\n",
      "Epoch 00057: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.1393 - acc: 0.9900 - val_loss: 0.1151 - val_acc: 0.9968\n",
      "Epoch 58/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.1088 - acc: 0.9936\n",
      "Epoch 00058: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.1088 - acc: 0.9936 - val_loss: 0.1083 - val_acc: 0.9840\n",
      "Epoch 59/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0913 - acc: 0.9936\n",
      "Epoch 00059: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0913 - acc: 0.9936 - val_loss: 0.0779 - val_acc: 0.9968\n",
      "Epoch 60/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0840 - acc: 0.9947\n",
      "Epoch 00060: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0840 - acc: 0.9947 - val_loss: 0.0646 - val_acc: 0.9968\n",
      "Epoch 61/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9946\n",
      "Epoch 00061: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0675 - acc: 0.9947 - val_loss: 0.0625 - val_acc: 0.9968\n",
      "Epoch 62/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9975\n",
      "Epoch 00062: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0612 - acc: 0.9975 - val_loss: 0.0560 - val_acc: 0.9968\n",
      "Epoch 63/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/88 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9975\n",
      "Epoch 00063: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0559 - acc: 0.9975 - val_loss: 0.0515 - val_acc: 0.9968\n",
      "Epoch 64/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0500 - acc: 0.9971\n",
      "Epoch 00064: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0500 - acc: 0.9971 - val_loss: 0.0477 - val_acc: 0.9968\n",
      "Epoch 65/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9985\n",
      "Epoch 00065: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0455 - acc: 0.9982 - val_loss: 0.0436 - val_acc: 0.9968\n",
      "Epoch 66/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9978\n",
      "Epoch 00066: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0400 - acc: 0.9979 - val_loss: 0.0388 - val_acc: 1.0000\n",
      "Epoch 67/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9978\n",
      "Epoch 00067: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0348 - acc: 0.9979 - val_loss: 0.0347 - val_acc: 1.0000\n",
      "Epoch 68/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0308 - acc: 0.9982\n",
      "Epoch 00068: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0308 - acc: 0.9982 - val_loss: 0.0316 - val_acc: 1.0000\n",
      "Epoch 69/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9986\n",
      "Epoch 00069: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0284 - acc: 0.9986 - val_loss: 0.0304 - val_acc: 1.0000\n",
      "Epoch 70/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9989- ETA: 0s - loss: 0.0269 - \n",
      "Epoch 00070: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0262 - acc: 0.9989 - val_loss: 0.0296 - val_acc: 1.0000\n",
      "Epoch 71/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0250 - acc: 0.9989\n",
      "Epoch 00071: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0250 - acc: 0.9989 - val_loss: 0.0296 - val_acc: 1.0000\n",
      "Epoch 72/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9989\n",
      "Epoch 00072: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0244 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 1.0000\n",
      "Epoch 73/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0231 - acc: 0.9993\n",
      "Epoch 00073: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0231 - acc: 0.9993 - val_loss: 0.0254 - val_acc: 1.0000\n",
      "Epoch 74/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9989\n",
      "Epoch 00074: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0228 - acc: 0.9989 - val_loss: 0.0249 - val_acc: 1.0000\n",
      "Epoch 75/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9985\n",
      "Epoch 00075: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0212 - acc: 0.9986 - val_loss: 0.0227 - val_acc: 1.0000\n",
      "Epoch 76/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9989- ETA: 1s \n",
      "Epoch 00076: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0199 - acc: 0.9989 - val_loss: 0.0219 - val_acc: 1.0000\n",
      "Epoch 77/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9986\n",
      "Epoch 00077: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0193 - acc: 0.9986 - val_loss: 0.0231 - val_acc: 1.0000\n",
      "Epoch 78/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9989\n",
      "Epoch 00078: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0182 - acc: 0.9989 - val_loss: 0.0197 - val_acc: 1.0000\n",
      "Epoch 79/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9986- ETA: 1s - loss: 0.0\n",
      "Epoch 00079: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0174 - acc: 0.9986 - val_loss: 0.0233 - val_acc: 1.0000\n",
      "Epoch 80/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9989- ETA: 0s - loss: 0.0159\n",
      "Epoch 00080: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0173 - acc: 0.9989 - val_loss: 0.0195 - val_acc: 1.0000\n",
      "Epoch 81/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9989\n",
      "Epoch 00081: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0167 - acc: 0.9989 - val_loss: 0.0223 - val_acc: 1.0000\n",
      "Epoch 82/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0156 - acc: 0.9993\n",
      "Epoch 00082: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0156 - acc: 0.9993 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 83/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9986\n",
      "Epoch 00083: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 0.0173 - acc: 0.9986 - val_loss: 0.0302 - val_acc: 1.0000\n",
      "Epoch 84/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0214 - acc: 0.9986\n",
      "Epoch 00084: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0214 - acc: 0.9986 - val_loss: 0.0275 - val_acc: 1.0000\n",
      "Epoch 85/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9975\n",
      "Epoch 00085: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0206 - acc: 0.9975 - val_loss: 0.0235 - val_acc: 1.0000\n",
      "Epoch 86/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0175 - acc: 0.9993\n",
      "Epoch 00086: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0175 - acc: 0.9993 - val_loss: 0.0215 - val_acc: 1.0000\n",
      "Epoch 87/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9989- ETA: 0s - loss: 0.0163 -\n",
      "Epoch 00087: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0163 - acc: 0.9989 - val_loss: 0.0196 - val_acc: 1.0000\n",
      "Epoch 88/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9993\n",
      "Epoch 00088: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0158 - acc: 0.9993 - val_loss: 0.0196 - val_acc: 1.0000\n",
      "Epoch 89/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0152 - acc: 0.9993\n",
      "Epoch 00089: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0152 - acc: 0.9993 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "Epoch 90/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9993\n",
      "Epoch 00090: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0139 - acc: 0.9989 - val_loss: 0.0175 - val_acc: 1.0000\n",
      "Epoch 91/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0974 - acc: 0.9979\n",
      "Epoch 00091: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0974 - acc: 0.9979 - val_loss: 0.0203 - val_acc: 1.0000\n",
      "Epoch 92/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0164 - acc: 0.9993\n",
      "Epoch 00092: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0164 - acc: 0.9993 - val_loss: 0.0221 - val_acc: 1.0000\n",
      "Epoch 93/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9993- ETA: 1s \n",
      "Epoch 00093: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0156 - acc: 0.9993 - val_loss: 0.0193 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9993\n",
      "Epoch 00094: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0153 - acc: 0.9993 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "Epoch 95/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9993\n",
      "Epoch 00095: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0139 - acc: 0.9993 - val_loss: 0.0162 - val_acc: 1.0000\n",
      "Epoch 96/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9996\n",
      "Epoch 00096: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0128 - acc: 0.9996 - val_loss: 0.0154 - val_acc: 1.0000\n",
      "Epoch 97/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0120 - acc: 0.9996\n",
      "Epoch 00097: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0120 - acc: 0.9996 - val_loss: 0.0159 - val_acc: 1.0000\n",
      "Epoch 98/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9993\n",
      "Epoch 00098: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0119 - acc: 0.9993 - val_loss: 0.0138 - val_acc: 1.0000\n",
      "Epoch 99/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0115 - acc: 0.9996\n",
      "Epoch 00099: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0115 - acc: 0.9996 - val_loss: 0.0154 - val_acc: 1.0000\n",
      "Epoch 100/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 1.0000\n",
      "Epoch 00100: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 1.0000\n",
      "Epoch 101/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 1.0000\n",
      "Epoch 00101: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.0131 - val_acc: 1.0000\n",
      "Epoch 102/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9996\n",
      "Epoch 00102: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0094 - acc: 0.9996 - val_loss: 0.0121 - val_acc: 1.0000\n",
      "Epoch 103/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0091 - acc: 0.9996\n",
      "Epoch 00103: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 1s 17ms/step - loss: 0.0091 - acc: 0.9996 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 104/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 00104: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.0105 - val_acc: 1.0000\n",
      "Epoch 105/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9996\n",
      "Epoch 00105: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0087 - acc: 0.9996 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "Epoch 106/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 00106: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.0104 - val_acc: 1.0000\n",
      "Epoch 107/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9996\n",
      "Epoch 00107: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0080 - acc: 0.9996 - val_loss: 0.0101 - val_acc: 1.0000\n",
      "Epoch 108/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 00108: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "Epoch 109/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 00109: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0103 - val_acc: 1.0000\n",
      "Epoch 110/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9996\n",
      "Epoch 00110: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0066 - acc: 0.9996 - val_loss: 0.0078 - val_acc: 1.0000\n",
      "Epoch 111/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0062 - acc: 0.9996- ETA: 0s - loss: 0.0062 - acc: 0\n",
      "Epoch 00111: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0062 - acc: 0.9996 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "Epoch 112/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0060 - acc: 0.9996\n",
      "Epoch 00112: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0060 - acc: 0.9996 - val_loss: 0.0081 - val_acc: 1.0000\n",
      "Epoch 113/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9996\n",
      "Epoch 00113: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0058 - acc: 0.9996 - val_loss: 0.0070 - val_acc: 1.0000\n",
      "Epoch 114/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 00114: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 115/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 00115: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 116/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 00116: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "Epoch 117/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9996\n",
      "Epoch 00117: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0047 - acc: 0.9996 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 118/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 00118: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 119/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 1.0000- ETA: 0s - loss: 0.004\n",
      "Epoch 00119: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "Epoch 120/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 00120: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 121/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 00121: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 122/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 00122: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 123/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9996\n",
      "Epoch 00123: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0040 - acc: 0.9996 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 124/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9996- ETA: 1s - lo\n",
      "Epoch 00124: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0037 - acc: 0.9996 - val_loss: 0.0047 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9996\n",
      "Epoch 00125: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0038 - acc: 0.9996 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 126/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 00126: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 127/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 00127: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 128/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 00128: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 129/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 00129: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 130/300\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 00130: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 131/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 00131: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 132/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 00132: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 133/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 00133: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 134/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 00134: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 135/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 00135: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 136/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0026 - acc: 1.0000- ETA: 1s - \n",
      "Epoch 00136: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 137/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 00137: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 138/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 00138: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 139/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 00139: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 140/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 00140: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 141/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 00141: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 142/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 00142: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 143/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 00143: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 144/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 00144: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 145/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 00145: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 146/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 00146: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 147/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 00147: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 148/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 00148: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 149/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 00149: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 150/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 00150: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 151/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 00151: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 152/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 00152: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 153/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 1.0000- ETA: 0s - loss: 0.0018 - a\n",
      "Epoch 00153: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 154/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 00154: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 155/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 00155: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - ETA: 0s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 00156: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 157/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 00157: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 158/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 1.0000 - ETA\n",
      "Epoch 00158: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 159/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 00159: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 160/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 00160: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 161/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 00161: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 162/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 00162: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 163/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 1.0000   \n",
      "Epoch 00163: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 164/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 00164: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 165/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 00165: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 166/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 9.6228e-04 - acc: 1.0000\n",
      "Epoch 00166: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 167/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 9.6556e-04 - acc: 1.0000\n",
      "Epoch 00167: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00167: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 9.5317e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 168/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 9.2697e-04 - acc: 1.0000\n",
      "Epoch 00168: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 9.1290e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 169/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 8.7703e-04 - acc: 1.0000\n",
      "Epoch 00169: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 8.7592e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 170/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 8.6168e-04 - acc: 1.0000\n",
      "Epoch 00170: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 8.8302e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 171/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 8.1111e-04 - acc: 1.0000\n",
      "Epoch 00171: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 8.1111e-04 - acc: 1.0000 - val_loss: 9.7213e-04 - val_acc: 1.0000\n",
      "Epoch 172/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 8.4210e-04 - acc: 1.0000\n",
      "Epoch 00172: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 8.3592e-04 - acc: 1.0000 - val_loss: 9.6808e-04 - val_acc: 1.0000\n",
      "Epoch 173/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 8.0287e-04 - acc: 1.0000\n",
      "Epoch 00173: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 7.9734e-04 - acc: 1.0000 - val_loss: 9.8065e-04 - val_acc: 1.0000\n",
      "Epoch 174/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 7.8542e-04 - acc: 1.0000\n",
      "Epoch 00174: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 7.8542e-04 - acc: 1.0000 - val_loss: 9.2846e-04 - val_acc: 1.0000\n",
      "Epoch 175/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 7.6408e-04 - acc: 1.0000\n",
      "Epoch 00175: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 7.7148e-04 - acc: 1.0000 - val_loss: 9.2354e-04 - val_acc: 1.0000\n",
      "Epoch 176/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 7.6061e-04 - acc: 1.0000\n",
      "Epoch 00176: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 3s 30ms/step - loss: 7.5646e-04 - acc: 1.0000 - val_loss: 8.7873e-04 - val_acc: 1.0000\n",
      "Epoch 177/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 7.4063e-04 - acc: 1.0000- ETA: 0s - loss: 7.2849e-04 - acc: 1.0\n",
      "Epoch 00177: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 7.3532e-04 - acc: 1.0000 - val_loss: 8.5966e-04 - val_acc: 1.0000\n",
      "Epoch 178/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 7.2220e-04 - acc: 1.0000\n",
      "Epoch 00178: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 7.1672e-04 - acc: 1.0000 - val_loss: 9.3027e-04 - val_acc: 1.0000\n",
      "Epoch 179/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 7.0498e-04 - acc: 1.0000\n",
      "Epoch 00179: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 7.0498e-04 - acc: 1.0000 - val_loss: 8.2351e-04 - val_acc: 1.0000\n",
      "Epoch 180/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 6.8400e-04 - acc: 1.0000\n",
      "Epoch 00180: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 6.8400e-04 - acc: 1.0000 - val_loss: 7.8638e-04 - val_acc: 1.0000\n",
      "Epoch 181/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 6.5780e-04 - acc: 1.0000\n",
      "Epoch 00181: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 6.5944e-04 - acc: 1.0000 - val_loss: 8.1788e-04 - val_acc: 1.0000\n",
      "Epoch 182/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 6.5413e-04 - acc: 1.0000\n",
      "Epoch 00182: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 6.5413e-04 - acc: 1.0000 - val_loss: 8.0725e-04 - val_acc: 1.0000\n",
      "Epoch 183/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 9.5537e-04 - acc: 1.0000\n",
      "Epoch 00183: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 9.3725e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 184/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9996\n",
      "Epoch 00184: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 185/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 8.0922e-04 - acc: 1.0000\n",
      "Epoch 00185: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 8.0569e-04 - acc: 1.0000 - val_loss: 8.6542e-04 - val_acc: 1.0000\n",
      "Epoch 186/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/88 [===========================>..] - ETA: 0s - loss: 6.9878e-04 - acc: 1.0000\n",
      "Epoch 00186: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 6.8788e-04 - acc: 1.0000 - val_loss: 8.1081e-04 - val_acc: 1.0000\n",
      "Epoch 187/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 6.4896e-04 - acc: 1.0000\n",
      "Epoch 00187: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 6.4458e-04 - acc: 1.0000 - val_loss: 7.6473e-04 - val_acc: 1.0000\n",
      "Epoch 188/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 5.9048e-04 - acc: 1.0000\n",
      "Epoch 00188: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 5.8215e-04 - acc: 1.0000 - val_loss: 7.1723e-04 - val_acc: 1.0000\n",
      "Epoch 189/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 5.3197e-04 - acc: 1.0000\n",
      "Epoch 00189: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 5.3197e-04 - acc: 1.0000 - val_loss: 7.3578e-04 - val_acc: 1.0000\n",
      "Epoch 190/300\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 5.2865e-04 - acc: 1.0000\n",
      "Epoch 00190: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 1s 17ms/step - loss: 5.2620e-04 - acc: 1.0000 - val_loss: 6.2301e-04 - val_acc: 1.0000\n",
      "Epoch 191/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 4.9385e-04 - acc: 1.0000\n",
      "Epoch 00191: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 1s 16ms/step - loss: 4.9385e-04 - acc: 1.0000 - val_loss: 5.8219e-04 - val_acc: 1.0000\n",
      "Epoch 192/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 4.8177e-04 - acc: 1.0000\n",
      "Epoch 00192: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 1s 16ms/step - loss: 4.7481e-04 - acc: 1.0000 - val_loss: 5.6000e-04 - val_acc: 1.0000\n",
      "Epoch 193/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 4.5824e-04 - acc: 1.0000- ETA: 0s - loss: 4.7912e-04 - ac\n",
      "Epoch 00193: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 4.6169e-04 - acc: 1.0000 - val_loss: 5.5630e-04 - val_acc: 1.0000\n",
      "Epoch 194/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 4.6144e-04 - acc: 1.0000\n",
      "Epoch 00194: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 4.7094e-04 - acc: 1.0000 - val_loss: 5.5606e-04 - val_acc: 1.0000\n",
      "Epoch 195/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 4.4055e-04 - acc: 1.0000\n",
      "Epoch 00195: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 4.4055e-04 - acc: 1.0000 - val_loss: 5.4316e-04 - val_acc: 1.0000\n",
      "Epoch 196/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 4.3657e-04 - acc: 1.0000\n",
      "Epoch 00196: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 4.3340e-04 - acc: 1.0000 - val_loss: 5.0712e-04 - val_acc: 1.0000\n",
      "Epoch 197/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 4.2039e-04 - acc: 1.0000\n",
      "Epoch 00197: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 4.2039e-04 - acc: 1.0000 - val_loss: 4.8764e-04 - val_acc: 1.0000\n",
      "Epoch 198/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 4.0256e-04 - acc: 1.0000\n",
      "Epoch 00198: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 4.0161e-04 - acc: 1.0000 - val_loss: 4.8211e-04 - val_acc: 1.0000\n",
      "Epoch 199/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.9131e-04 - acc: 1.0000\n",
      "Epoch 00199: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 3.9131e-04 - acc: 1.0000 - val_loss: 4.6094e-04 - val_acc: 1.0000\n",
      "Epoch 200/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 3.8965e-04 - acc: 1.0000\n",
      "Epoch 00200: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 3.8358e-04 - acc: 1.0000 - val_loss: 4.7219e-04 - val_acc: 1.0000\n",
      "Epoch 201/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 3.7031e-04 - acc: 1.0000\n",
      "Epoch 00201: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 3.7054e-04 - acc: 1.0000 - val_loss: 4.3479e-04 - val_acc: 1.0000\n",
      "Epoch 202/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 3.7867e-04 - acc: 1.0000- ETA: 0s - loss: 3.6321e-04 - acc: 1\n",
      "Epoch 00202: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 3.7810e-04 - acc: 1.0000 - val_loss: 5.2182e-04 - val_acc: 1.0000\n",
      "Epoch 203/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 3.6874e-04 - acc: 1.0000\n",
      "Epoch 00203: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 3.6618e-04 - acc: 1.0000 - val_loss: 4.1982e-04 - val_acc: 1.0000\n",
      "Epoch 204/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 3.5048e-04 - acc: 1.0000\n",
      "Epoch 00204: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 3.4621e-04 - acc: 1.0000 - val_loss: 3.9219e-04 - val_acc: 1.0000\n",
      "Epoch 205/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.3493e-04 - acc: 1.0000\n",
      "Epoch 00205: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 20ms/step - loss: 3.3493e-04 - acc: 1.0000 - val_loss: 3.9410e-04 - val_acc: 1.0000\n",
      "Epoch 206/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 3.1745e-04 - acc: 1.0000\n",
      "Epoch 00206: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 3.1694e-04 - acc: 1.0000 - val_loss: 3.7517e-04 - val_acc: 1.0000\n",
      "Epoch 207/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 3.1029e-04 - acc: 1.0000\n",
      "Epoch 00207: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 3.1029e-04 - acc: 1.0000 - val_loss: 3.5811e-04 - val_acc: 1.0000\n",
      "Epoch 208/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 3.0449e-04 - acc: 1.0000\n",
      "Epoch 00208: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 17ms/step - loss: 3.0380e-04 - acc: 1.0000 - val_loss: 3.5699e-04 - val_acc: 1.0000\n",
      "Epoch 209/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 2.9030e-04 - acc: 1.0000\n",
      "Epoch 00209: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 1s 16ms/step - loss: 2.8542e-04 - acc: 1.0000 - val_loss: 3.4166e-04 - val_acc: 1.0000\n",
      "Epoch 210/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.7977e-04 - acc: 1.0000\n",
      "Epoch 00210: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 2.7977e-04 - acc: 1.0000 - val_loss: 3.3279e-04 - val_acc: 1.0000\n",
      "Epoch 211/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.6065e-04 - acc: 1.0000- ETA: 0s - loss: 2.431\n",
      "Epoch 00211: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 2.6601e-04 - acc: 1.0000 - val_loss: 3.1584e-04 - val_acc: 1.0000\n",
      "Epoch 212/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.6210e-04 - acc: 1.0000\n",
      "Epoch 00212: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 2.6027e-04 - acc: 1.0000 - val_loss: 3.2722e-04 - val_acc: 1.0000\n",
      "Epoch 213/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 2.5185e-04 - acc: 1.0000\n",
      "Epoch 00213: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 2.5161e-04 - acc: 1.0000 - val_loss: 3.0990e-04 - val_acc: 1.0000\n",
      "Epoch 214/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 2.4707e-04 - acc: 1.0000\n",
      "Epoch 00214: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 2.4345e-04 - acc: 1.0000 - val_loss: 2.7835e-04 - val_acc: 1.0000\n",
      "Epoch 215/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.3771e-04 - acc: 1.0000\n",
      "Epoch 00215: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 2.3590e-04 - acc: 1.0000 - val_loss: 2.7691e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.3268e-04 - acc: 1.0000\n",
      "Epoch 00216: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 2.3268e-04 - acc: 1.0000 - val_loss: 3.5790e-04 - val_acc: 1.0000\n",
      "Epoch 217/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 2.2691e-04 - acc: 1.0000\n",
      "Epoch 00217: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00217: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 2.2561e-04 - acc: 1.0000 - val_loss: 2.4164e-04 - val_acc: 1.0000\n",
      "Epoch 218/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.1073e-04 - acc: 1.0000\n",
      "Epoch 00218: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 2.1073e-04 - acc: 1.0000 - val_loss: 2.5968e-04 - val_acc: 1.0000\n",
      "Epoch 219/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.0472e-04 - acc: 1.0000\n",
      "Epoch 00219: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 2.0472e-04 - acc: 1.0000 - val_loss: 2.4099e-04 - val_acc: 1.0000\n",
      "Epoch 220/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.0207e-04 - acc: 1.0000\n",
      "Epoch 00220: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 2.0207e-04 - acc: 1.0000 - val_loss: 2.4529e-04 - val_acc: 1.0000\n",
      "Epoch 221/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.9101e-04 - acc: 1.0000- ETA: 1s - loss: 1.9526e-04 - ETA: 0s - loss: 1.9011e-04 - acc: \n",
      "Epoch 00221: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.9101e-04 - acc: 1.0000 - val_loss: 2.3312e-04 - val_acc: 1.0000\n",
      "Epoch 222/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.8367e-04 - acc: 1.0000\n",
      "Epoch 00222: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.9267e-04 - acc: 1.0000 - val_loss: 2.3384e-04 - val_acc: 1.0000\n",
      "Epoch 223/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.8897e-04 - acc: 1.0000\n",
      "Epoch 00223: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.8779e-04 - acc: 1.0000 - val_loss: 2.1985e-04 - val_acc: 1.0000\n",
      "Epoch 224/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.8725e-04 - acc: 1.0000\n",
      "Epoch 00224: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.8543e-04 - acc: 1.0000 - val_loss: 2.1718e-04 - val_acc: 1.0000\n",
      "Epoch 225/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.7854e-04 - acc: 1.0000\n",
      "Epoch 00225: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.7925e-04 - acc: 1.0000 - val_loss: 2.1217e-04 - val_acc: 1.0000\n",
      "Epoch 226/300\n",
      "85/88 [===========================>..] - ETA: 0s - loss: 1.7157e-04 - acc: 1.0000\n",
      "Epoch 00226: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 20ms/step - loss: 1.7754e-04 - acc: 1.0000 - val_loss: 2.1168e-04 - val_acc: 1.0000\n",
      "Epoch 227/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6869e-04 - acc: 1.0000\n",
      "Epoch 00227: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 1s 15ms/step - loss: 1.6869e-04 - acc: 1.0000 - val_loss: 2.0003e-04 - val_acc: 1.0000\n",
      "Epoch 228/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.7140e-04 - acc: 1.0000\n",
      "Epoch 00228: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 1.7010e-04 - acc: 1.0000 - val_loss: 1.9865e-04 - val_acc: 1.0000\n",
      "Epoch 229/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6268e-04 - acc: 1.0000\n",
      "Epoch 00229: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.6268e-04 - acc: 1.0000 - val_loss: 1.9252e-04 - val_acc: 1.0000\n",
      "Epoch 230/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6792e-04 - acc: 1.0000\n",
      "Epoch 00230: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.6792e-04 - acc: 1.0000 - val_loss: 1.9654e-04 - val_acc: 1.0000\n",
      "Epoch 231/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.5823e-04 - acc: 1.0000\n",
      "Epoch 00231: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.5572e-04 - acc: 1.0000 - val_loss: 1.8726e-04 - val_acc: 1.0000\n",
      "Epoch 232/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5591e-04 - acc: 1.0000\n",
      "Epoch 00232: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.5591e-04 - acc: 1.0000 - val_loss: 1.8566e-04 - val_acc: 1.0000\n",
      "Epoch 233/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.5395e-04 - acc: 1.0000- ETA: 0s - loss: 1.4240e-04 - acc: 1\n",
      "Epoch 00233: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.5197e-04 - acc: 1.0000 - val_loss: 1.8695e-04 - val_acc: 1.0000\n",
      "Epoch 234/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.4446e-04 - acc: 1.0000\n",
      "Epoch 00234: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.4714e-04 - acc: 1.0000 - val_loss: 1.8192e-04 - val_acc: 1.0000\n",
      "Epoch 235/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.4693e-04 - acc: 1.0000\n",
      "Epoch 00235: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.4565e-04 - acc: 1.0000 - val_loss: 1.7686e-04 - val_acc: 1.0000\n",
      "Epoch 236/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.4225e-04 - acc: 1.0000\n",
      "Epoch 00236: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.4120e-04 - acc: 1.0000 - val_loss: 1.7589e-04 - val_acc: 1.0000\n",
      "Epoch 237/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.3736e-04 - acc: 1.0000\n",
      "Epoch 00237: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.3692e-04 - acc: 1.0000 - val_loss: 1.5676e-04 - val_acc: 1.0000\n",
      "Epoch 238/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.3372e-04 - acc: 1.0000\n",
      "Epoch 00238: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.3372e-04 - acc: 1.0000 - val_loss: 1.5286e-04 - val_acc: 1.0000\n",
      "Epoch 239/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.2422e-04 - acc: 1.0000\n",
      "Epoch 00239: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.2804e-04 - acc: 1.0000 - val_loss: 1.5100e-04 - val_acc: 1.0000\n",
      "Epoch 240/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.2748e-04 - acc: 1.0000\n",
      "Epoch 00240: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.2684e-04 - acc: 1.0000 - val_loss: 1.5420e-04 - val_acc: 1.0000\n",
      "Epoch 241/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.2406e-04 - acc: 1.0000\n",
      "Epoch 00241: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.2328e-04 - acc: 1.0000 - val_loss: 1.4262e-04 - val_acc: 1.0000\n",
      "Epoch 242/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.2185e-04 - acc: 1.0000\n",
      "Epoch 00242: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.2035e-04 - acc: 1.0000 - val_loss: 1.3704e-04 - val_acc: 1.0000\n",
      "Epoch 243/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.2131e-04 - acc: 1.0000\n",
      "Epoch 00243: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.2131e-04 - acc: 1.0000 - val_loss: 1.3342e-04 - val_acc: 1.0000\n",
      "Epoch 244/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.1208e-04 - acc: 1.0000\n",
      "Epoch 00244: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.1208e-04 - acc: 1.0000 - val_loss: 1.2894e-04 - val_acc: 1.0000\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/88 [============================>.] - ETA: 0s - loss: 1.1040e-04 - acc: 1.0000\n",
      "Epoch 00245: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0956e-04 - acc: 1.0000 - val_loss: 1.2765e-04 - val_acc: 1.0000\n",
      "Epoch 246/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.0240e-04 - acc: 1.0000\n",
      "Epoch 00246: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0631e-04 - acc: 1.0000 - val_loss: 1.2076e-04 - val_acc: 1.0000\n",
      "Epoch 247/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.0543e-04 - acc: 1.0000\n",
      "Epoch 00247: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0487e-04 - acc: 1.0000 - val_loss: 1.1757e-04 - val_acc: 1.0000\n",
      "Epoch 248/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.0174e-04 - acc: 1.0000\n",
      "Epoch 00248: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0174e-04 - acc: 1.0000 - val_loss: 1.1291e-04 - val_acc: 1.0000\n",
      "Epoch 249/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 9.7968e-05 - acc: 1.0000\n",
      "Epoch 00249: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 9.7241e-05 - acc: 1.0000 - val_loss: 1.0975e-04 - val_acc: 1.0000\n",
      "Epoch 250/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 9.4755e-05 - acc: 1.0000\n",
      "Epoch 00250: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 9.4512e-05 - acc: 1.0000 - val_loss: 1.0788e-04 - val_acc: 1.0000\n",
      "Epoch 251/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 9.4177e-05 - acc: 1.0000- ETA: 0s - loss: 8.5027e-05\n",
      "Epoch 00251: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 9.4177e-05 - acc: 1.0000 - val_loss: 1.0381e-04 - val_acc: 1.0000\n",
      "Epoch 252/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 9.2526e-05 - acc: 1.0000\n",
      "Epoch 00252: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 9.2377e-05 - acc: 1.0000 - val_loss: 1.1639e-04 - val_acc: 1.0000\n",
      "Epoch 253/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.1639e-04 - acc: 1.0000\n",
      "Epoch 00253: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.1639e-04 - acc: 1.0000 - val_loss: 1.0738e-04 - val_acc: 1.0000\n",
      "Epoch 254/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 9.0942e-05 - acc: 1.0000\n",
      "Epoch 00254: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 8.9683e-05 - acc: 1.0000 - val_loss: 1.0214e-04 - val_acc: 1.0000\n",
      "Epoch 255/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.2355e-04 - acc: 1.0000\n",
      "Epoch 00255: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 2.2314e-04 - acc: 1.0000 - val_loss: 1.3852e-04 - val_acc: 1.0000\n",
      "Epoch 256/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 00256: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0021 - acc: 0.9996 - val_loss: 4.0855e-04 - val_acc: 1.0000\n",
      "Epoch 257/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 2.9753e-04 - acc: 1.0000\n",
      "Epoch 00257: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 2.9578e-04 - acc: 1.0000 - val_loss: 2.2990e-04 - val_acc: 1.0000\n",
      "Epoch 258/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 2.0564e-04 - acc: 1.0000\n",
      "Epoch 00258: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 2.0461e-04 - acc: 1.0000 - val_loss: 2.0488e-04 - val_acc: 1.0000\n",
      "Epoch 259/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.6230e-04 - acc: 1.0000\n",
      "Epoch 00259: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.6124e-04 - acc: 1.0000 - val_loss: 1.8140e-04 - val_acc: 1.0000\n",
      "Epoch 260/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.4529e-04 - acc: 1.0000\n",
      "Epoch 00260: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.4529e-04 - acc: 1.0000 - val_loss: 1.6346e-04 - val_acc: 1.0000\n",
      "Epoch 261/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.3699e-04 - acc: 1.0000\n",
      "Epoch 00261: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.3607e-04 - acc: 1.0000 - val_loss: 1.5371e-04 - val_acc: 1.0000\n",
      "Epoch 262/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.3140e-04 - acc: 1.0000\n",
      "Epoch 00262: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.3041e-04 - acc: 1.0000 - val_loss: 1.4531e-04 - val_acc: 1.0000\n",
      "Epoch 263/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.2658e-04 - acc: 1.0000\n",
      "Epoch 00263: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 24ms/step - loss: 1.2562e-04 - acc: 1.0000 - val_loss: 1.3994e-04 - val_acc: 1.0000\n",
      "Epoch 264/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.2217e-04 - acc: 1.0000\n",
      "Epoch 00264: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 1.2124e-04 - acc: 1.0000 - val_loss: 1.3463e-04 - val_acc: 1.0000\n",
      "Epoch 265/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.2005e-04 - acc: 1.0000\n",
      "Epoch 00265: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.1958e-04 - acc: 1.0000 - val_loss: 1.3205e-04 - val_acc: 1.0000\n",
      "Epoch 266/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.1883e-04 - acc: 1.0000\n",
      "Epoch 00266: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.1764e-04 - acc: 1.0000 - val_loss: 1.2706e-04 - val_acc: 1.0000\n",
      "Epoch 267/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.1447e-04 - acc: 1.0000\n",
      "Epoch 00267: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00267: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.1447e-04 - acc: 1.0000 - val_loss: 1.2352e-04 - val_acc: 1.0000\n",
      "Epoch 268/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.1143e-04 - acc: 1.0000\n",
      "Epoch 00268: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.1143e-04 - acc: 1.0000 - val_loss: 1.2196e-04 - val_acc: 1.0000\n",
      "Epoch 269/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.1193e-04 - acc: 1.0000\n",
      "Epoch 00269: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.1031e-04 - acc: 1.0000 - val_loss: 1.2077e-04 - val_acc: 1.0000\n",
      "Epoch 270/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.0928e-04 - acc: 1.0000- ETA: 0s - loss: 1.0932e-04 - acc: \n",
      "Epoch 00270: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0873e-04 - acc: 1.0000 - val_loss: 1.1982e-04 - val_acc: 1.0000\n",
      "Epoch 271/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.0811e-04 - acc: 1.0000\n",
      "Epoch 00271: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0811e-04 - acc: 1.0000 - val_loss: 1.1796e-04 - val_acc: 1.0000\n",
      "Epoch 272/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.0799e-04 - acc: 1.0000\n",
      "Epoch 00272: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0687e-04 - acc: 1.0000 - val_loss: 1.1618e-04 - val_acc: 1.0000\n",
      "Epoch 273/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.0659e-04 - acc: 1.0000\n",
      "Epoch 00273: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0659e-04 - acc: 1.0000 - val_loss: 1.1496e-04 - val_acc: 1.0000\n",
      "Epoch 274/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/88 [============================>.] - ETA: 0s - loss: 9.9665e-05 - acc: 1.0000- ETA: 0s - loss: 7.4157e-05 \n",
      "Epoch 00274: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0381e-04 - acc: 1.0000 - val_loss: 1.1333e-04 - val_acc: 1.0000\n",
      "Epoch 275/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 1.0464e-04 - acc: 1.0000\n",
      "Epoch 00275: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0316e-04 - acc: 1.0000 - val_loss: 1.1222e-04 - val_acc: 1.0000\n",
      "Epoch 276/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.0162e-04 - acc: 1.0000- ETA: 0s - loss: 1.0830e-04 - acc:  - ETA: 0s - loss: 1.0258e-04 - acc: 1.000\n",
      "Epoch 00276: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 1.0162e-04 - acc: 1.0000 - val_loss: 1.1044e-04 - val_acc: 1.0000\n",
      "Epoch 277/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 1.0107e-04 - acc: 1.0000\n",
      "Epoch 00277: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 1.0057e-04 - acc: 1.0000 - val_loss: 1.0969e-04 - val_acc: 1.0000\n",
      "Epoch 278/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 9.9915e-05 - acc: 1.0000\n",
      "Epoch 00278: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 9.9915e-05 - acc: 1.0000 - val_loss: 1.0726e-04 - val_acc: 1.0000\n",
      "Epoch 279/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 9.8393e-05 - acc: 1.0000\n",
      "Epoch 00279: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 9.8496e-05 - acc: 1.0000 - val_loss: 1.0665e-04 - val_acc: 1.0000\n",
      "Epoch 280/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 9.7269e-05 - acc: 1.0000\n",
      "Epoch 00280: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 9.6586e-05 - acc: 1.0000 - val_loss: 1.0485e-04 - val_acc: 1.0000\n",
      "Epoch 281/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 9.5527e-05 - acc: 1.0000\n",
      "Epoch 00281: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 9.5339e-05 - acc: 1.0000 - val_loss: 1.0364e-04 - val_acc: 1.0000\n",
      "Epoch 282/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 9.4899e-05 - acc: 1.0000- ETA: 0s - loss: 8.7196e-05 - ac\n",
      "Epoch 00282: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 9.4235e-05 - acc: 1.0000 - val_loss: 1.0183e-04 - val_acc: 1.0000\n",
      "Epoch 283/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 9.4385e-05 - acc: 1.0000\n",
      "Epoch 00283: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 9.4311e-05 - acc: 1.0000 - val_loss: 1.0226e-04 - val_acc: 1.0000\n",
      "Epoch 284/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 9.2385e-05 - acc: 1.0000- ETA: 1s - loss: \n",
      "Epoch 00284: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 9.1780e-05 - acc: 1.0000 - val_loss: 9.9384e-05 - val_acc: 1.0000\n",
      "Epoch 285/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 9.2002e-05 - acc: 1.0000\n",
      "Epoch 00285: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 9.0838e-05 - acc: 1.0000 - val_loss: 9.7695e-05 - val_acc: 1.0000\n",
      "Epoch 286/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 8.8743e-05 - acc: 1.0000- ETA: 0s - loss: 8.2955e-05 - ac\n",
      "Epoch 00286: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 8.8743e-05 - acc: 1.0000 - val_loss: 9.5986e-05 - val_acc: 1.0000\n",
      "Epoch 287/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 8.7585e-05 - acc: 1.0000- ETA: 0s - loss: 8.6779e-05 - acc: 1.0\n",
      "Epoch 00287: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 8.7585e-05 - acc: 1.0000 - val_loss: 9.4108e-05 - val_acc: 1.0000\n",
      "Epoch 288/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 8.7232e-05 - acc: 1.0000\n",
      "Epoch 00288: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 25ms/step - loss: 8.6662e-05 - acc: 1.0000 - val_loss: 9.4074e-05 - val_acc: 1.0000\n",
      "Epoch 289/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 8.5497e-05 - acc: 1.0000\n",
      "Epoch 00289: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 8.5497e-05 - acc: 1.0000 - val_loss: 9.0889e-05 - val_acc: 1.0000\n",
      "Epoch 290/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 8.4607e-05 - acc: 1.0000- ETA: 1s - loss: 9.8272e-0\n",
      "Epoch 00290: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 8.3559e-05 - acc: 1.0000 - val_loss: 8.9031e-05 - val_acc: 1.0000\n",
      "Epoch 291/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 8.1232e-05 - acc: 1.0000\n",
      "Epoch 00291: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 8.2008e-05 - acc: 1.0000 - val_loss: 8.8268e-05 - val_acc: 1.0000\n",
      "Epoch 292/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 8.0577e-05 - acc: 1.0000\n",
      "Epoch 00292: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 7.9982e-05 - acc: 1.0000 - val_loss: 8.6593e-05 - val_acc: 1.0000\n",
      "Epoch 293/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 7.8851e-05 - acc: 1.0000\n",
      "Epoch 00293: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 7.8576e-05 - acc: 1.0000 - val_loss: 8.5645e-05 - val_acc: 1.0000\n",
      "Epoch 294/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 7.7043e-05 - acc: 1.0000\n",
      "Epoch 00294: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 7.7179e-05 - acc: 1.0000 - val_loss: 8.2942e-05 - val_acc: 1.0000\n",
      "Epoch 295/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 7.6099e-05 - acc: 1.0000\n",
      "Epoch 00295: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 7.5552e-05 - acc: 1.0000 - val_loss: 8.1753e-05 - val_acc: 1.0000\n",
      "Epoch 296/300\n",
      "87/88 [============================>.] - ETA: 0s - loss: 7.5118e-05 - acc: 1.0000\n",
      "Epoch 00296: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 7.4563e-05 - acc: 1.0000 - val_loss: 8.0787e-05 - val_acc: 1.0000\n",
      "Epoch 297/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 7.1926e-05 - acc: 1.0000\n",
      "Epoch 00297: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 7.3077e-05 - acc: 1.0000 - val_loss: 8.0704e-05 - val_acc: 1.0000\n",
      "Epoch 298/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 7.1182e-05 - acc: 1.0000- ETA: 1s - loss: 9.0542\n",
      "Epoch 00298: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 7.1182e-05 - acc: 1.0000 - val_loss: 7.7092e-05 - val_acc: 1.0000\n",
      "Epoch 299/300\n",
      "88/88 [==============================] - ETA: 0s - loss: 6.9692e-05 - acc: 1.0000\n",
      "Epoch 00299: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 6.9692e-05 - acc: 1.0000 - val_loss: 7.5287e-05 - val_acc: 1.0000\n",
      "Epoch 300/300\n",
      "86/88 [============================>.] - ETA: 0s - loss: 6.8172e-05 - acc: 1.0000\n",
      "Epoch 00300: val_acc did not improve from 1.00000\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 6.7908e-05 - acc: 1.0000 - val_loss: 7.3282e-05 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=300,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model2_1.0.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[195,   0],\n",
       "        [  0, 117]],\n",
       "\n",
       "       [[205,   0],\n",
       "        [  0, 107]],\n",
       "\n",
       "       [[224,   0],\n",
       "        [  0,  88]]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model2_1.0.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
