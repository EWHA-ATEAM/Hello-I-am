{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_classifier(hand_image).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_hi_dir = os.path.join('/content/drive/MyDrive/dataset/train/hi')\n",
        "train_heart_dir = os.path.join('/content/drive/MyDrive/dataset/train/heart')\n",
        "train_pet_dir = os.path.join('/content/drive/MyDrive/dataset/train/pet')\n",
        "\n",
        "train_hi_files = os.listdir(train_hi_dir)\n",
        "train_heart_files = os.listdir(train_heart_dir)\n",
        "train_pet_files = os.listdir(train_pet_dir)\n",
        "\n",
        "print('total num of training hi images:', len(train_hi_files))\n",
        "print('total num of training heart images:', len(train_heart_files))\n",
        "print('total num of training pet images:', len(train_pet_files))\n",
        "\n",
        "print(train_hi_files[:10])\n",
        "print(train_heart_files[:10])\n",
        "print(train_pet_files[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFuIH3mdxq1l",
        "outputId": "2ebc87f2-917f-466c-c0cd-e657f3de1eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total num of training hi images: 756\n",
            "total num of training heart images: 815\n",
            "total num of training pet images: 681\n",
            "['IMG_6886.jpg', 'IMG_6884.jpg', 'IMG_6885.jpg', 'IMG_6883.jpg', 'IMG_6889.jpg', 'IMG_6888.jpg', 'IMG_6887.jpg', 'IMG_6890.jpg', 'IMG_6891.jpg', 'IMG_6893.jpg']\n",
            "['KakaoTalk_20220305_205657105.jpg', 'KakaoTalk_20220305_205659181.jpg', 'KakaoTalk_20220305_205656630.jpg', 'KakaoTalk_20220305_205658658.jpg', 'KakaoTalk_20220305_205701290.jpg', 'KakaoTalk_20220305_205702338.jpg', 'KakaoTalk_20220305_205700760.jpg', 'KakaoTalk_20220305_205701840.jpg', 'KakaoTalk_20220305_205703381.jpg', 'KakaoTalk_20220305_205700235.jpg']\n",
            "['IMG_7335.JPG', 'IMG_7340.JPG', 'IMG_7342.JPG', 'IMG_7345.JPG', 'IMG_7344.JPG', 'IMG_7336.JPG', 'IMG_7346.JPG', 'IMG_7343.JPG', 'IMG_7341.JPG', 'IMG_7347.JPG']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAINING_DIR = \"/content/drive/MyDrive/dataset/train/\"\n",
        "training_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "\n",
        "VALIDATION_DIR = \"/content/drive/MyDrive/dataset/validation/\"\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "TRAINING_DIR,\n",
        "target_size=(150, 150),\n",
        "class_mode='categorical',\n",
        "batch_size=80\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "VALIDATION_DIR,\n",
        "target_size=(150, 150),\n",
        "class_mode='categorical',\n",
        "batch_size=80\n",
        ")\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "  # This is the first convolution\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  # The second convolution\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  # The third convolution\n",
        "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  # The fourth convolution\n",
        "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  # Flatten the results to feed into a DNN\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  # 512 neuron hidden layer\n",
        "  tf.keras.layers.Dense(512, activation='relu'),\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, epochs=52, steps_per_epoch=15, validation_data = validation_generator, verbose = 2, validation_steps=3)\n",
        "\n",
        "model.save(\"model2.h5\")\n",
        "\n",
        "#로컬에 저장하기\n",
        "from google.colab import files\n",
        "files.download('model2.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XUetd6Yuyq9V",
        "outputId": "e182a843-3026-46ca-cd71-a9f1d1313217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2252 images belonging to 3 classes.\n",
            "Found 150 images belonging to 3 classes.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               3211776   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,473,475\n",
            "Trainable params: 3,473,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/52\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3 batches). You may need to use the repeat() function when building your dataset.\n",
            "15/15 - 457s - loss: 2.4681 - accuracy: 0.3357 - val_loss: 1.0984 - val_accuracy: 0.3333 - 457s/epoch - 30s/step\n",
            "Epoch 2/52\n",
            "15/15 - 326s - loss: 1.0978 - accuracy: 0.3392 - 326s/epoch - 22s/step\n",
            "Epoch 3/52\n",
            "15/15 - 261s - loss: 1.0864 - accuracy: 0.3892 - 261s/epoch - 17s/step\n",
            "Epoch 4/52\n",
            "15/15 - 242s - loss: 1.1291 - accuracy: 0.3658 - 242s/epoch - 16s/step\n",
            "Epoch 5/52\n",
            "15/15 - 233s - loss: 1.0948 - accuracy: 0.3867 - 233s/epoch - 16s/step\n",
            "Epoch 6/52\n",
            "15/15 - 226s - loss: 1.0811 - accuracy: 0.3825 - 226s/epoch - 15s/step\n",
            "Epoch 7/52\n",
            "15/15 - 208s - loss: 1.1579 - accuracy: 0.4028 - 208s/epoch - 14s/step\n",
            "Epoch 8/52\n",
            "15/15 - 215s - loss: 1.0573 - accuracy: 0.4143 - 215s/epoch - 14s/step\n",
            "Epoch 9/52\n",
            "15/15 - 210s - loss: 1.1210 - accuracy: 0.4585 - 210s/epoch - 14s/step\n",
            "Epoch 10/52\n",
            "15/15 - 212s - loss: 1.0610 - accuracy: 0.4479 - 212s/epoch - 14s/step\n",
            "Epoch 11/52\n",
            "15/15 - 220s - loss: 0.9756 - accuracy: 0.5025 - 220s/epoch - 15s/step\n",
            "Epoch 12/52\n",
            "15/15 - 222s - loss: 0.9579 - accuracy: 0.5100 - 222s/epoch - 15s/step\n",
            "Epoch 13/52\n",
            "15/15 - 216s - loss: 0.9482 - accuracy: 0.5371 - 216s/epoch - 14s/step\n",
            "Epoch 14/52\n",
            "15/15 - 229s - loss: 0.8954 - accuracy: 0.5842 - 229s/epoch - 15s/step\n",
            "Epoch 15/52\n",
            "15/15 - 229s - loss: 0.9070 - accuracy: 0.5783 - 229s/epoch - 15s/step\n",
            "Epoch 16/52\n",
            "15/15 - 216s - loss: 0.9907 - accuracy: 0.5901 - 216s/epoch - 14s/step\n",
            "Epoch 17/52\n",
            "15/15 - 218s - loss: 0.7696 - accuracy: 0.6749 - 218s/epoch - 15s/step\n",
            "Epoch 18/52\n",
            "15/15 - 213s - loss: 0.7850 - accuracy: 0.6617 - 213s/epoch - 14s/step\n",
            "Epoch 19/52\n",
            "15/15 - 227s - loss: 0.7366 - accuracy: 0.7000 - 227s/epoch - 15s/step\n",
            "Epoch 20/52\n",
            "15/15 - 234s - loss: 0.6654 - accuracy: 0.7200 - 234s/epoch - 16s/step\n",
            "Epoch 21/52\n",
            "15/15 - 211s - loss: 0.6483 - accuracy: 0.7288 - 211s/epoch - 14s/step\n",
            "Epoch 22/52\n",
            "15/15 - 222s - loss: 0.6144 - accuracy: 0.7412 - 222s/epoch - 15s/step\n",
            "Epoch 23/52\n",
            "15/15 - 223s - loss: 0.5150 - accuracy: 0.8033 - 223s/epoch - 15s/step\n",
            "Epoch 24/52\n",
            "15/15 - 211s - loss: 0.5360 - accuracy: 0.7765 - 211s/epoch - 14s/step\n",
            "Epoch 25/52\n",
            "15/15 - 230s - loss: 0.4452 - accuracy: 0.8075 - 230s/epoch - 15s/step\n",
            "Epoch 26/52\n",
            "15/15 - 215s - loss: 0.5595 - accuracy: 0.7880 - 215s/epoch - 14s/step\n",
            "Epoch 27/52\n",
            "15/15 - 217s - loss: 0.4792 - accuracy: 0.7915 - 217s/epoch - 14s/step\n",
            "Epoch 28/52\n",
            "15/15 - 211s - loss: 0.4276 - accuracy: 0.8357 - 211s/epoch - 14s/step\n",
            "Epoch 29/52\n",
            "15/15 - 227s - loss: 0.4571 - accuracy: 0.8225 - 227s/epoch - 15s/step\n",
            "Epoch 30/52\n",
            "15/15 - 230s - loss: 0.3694 - accuracy: 0.8533 - 230s/epoch - 15s/step\n",
            "Epoch 31/52\n",
            "15/15 - 224s - loss: 0.3981 - accuracy: 0.8317 - 224s/epoch - 15s/step\n",
            "Epoch 32/52\n",
            "15/15 - 230s - loss: 0.3234 - accuracy: 0.8642 - 230s/epoch - 15s/step\n",
            "Epoch 33/52\n",
            "15/15 - 212s - loss: 0.3643 - accuracy: 0.8595 - 212s/epoch - 14s/step\n",
            "Epoch 34/52\n",
            "15/15 - 223s - loss: 0.3304 - accuracy: 0.8700 - 223s/epoch - 15s/step\n",
            "Epoch 35/52\n",
            "15/15 - 228s - loss: 0.3304 - accuracy: 0.8892 - 228s/epoch - 15s/step\n",
            "Epoch 36/52\n",
            "15/15 - 225s - loss: 0.3357 - accuracy: 0.8692 - 225s/epoch - 15s/step\n",
            "Epoch 37/52\n",
            "15/15 - 232s - loss: 0.2970 - accuracy: 0.8958 - 232s/epoch - 15s/step\n",
            "Epoch 38/52\n",
            "15/15 - 212s - loss: 0.2358 - accuracy: 0.9055 - 212s/epoch - 14s/step\n",
            "Epoch 39/52\n",
            "15/15 - 209s - loss: 0.4820 - accuracy: 0.8313 - 209s/epoch - 14s/step\n",
            "Epoch 40/52\n",
            "15/15 - 227s - loss: 0.2275 - accuracy: 0.9225 - 227s/epoch - 15s/step\n",
            "Epoch 41/52\n",
            "15/15 - 208s - loss: 0.2719 - accuracy: 0.9046 - 208s/epoch - 14s/step\n",
            "Epoch 42/52\n",
            "15/15 - 227s - loss: 0.3080 - accuracy: 0.8783 - 227s/epoch - 15s/step\n",
            "Epoch 43/52\n",
            "15/15 - 224s - loss: 0.1980 - accuracy: 0.9225 - 224s/epoch - 15s/step\n",
            "Epoch 44/52\n",
            "15/15 - 211s - loss: 0.2002 - accuracy: 0.9249 - 211s/epoch - 14s/step\n",
            "Epoch 45/52\n",
            "15/15 - 227s - loss: 0.2168 - accuracy: 0.9183 - 227s/epoch - 15s/step\n",
            "Epoch 46/52\n",
            "15/15 - 213s - loss: 0.2446 - accuracy: 0.9205 - 213s/epoch - 14s/step\n",
            "Epoch 47/52\n",
            "15/15 - 222s - loss: 0.1981 - accuracy: 0.9258 - 222s/epoch - 15s/step\n",
            "Epoch 48/52\n",
            "15/15 - 229s - loss: 0.3090 - accuracy: 0.8942 - 229s/epoch - 15s/step\n",
            "Epoch 49/52\n",
            "15/15 - 226s - loss: 0.2216 - accuracy: 0.9117 - 226s/epoch - 15s/step\n",
            "Epoch 50/52\n",
            "15/15 - 236s - loss: 0.1766 - accuracy: 0.9417 - 236s/epoch - 16s/step\n",
            "Epoch 51/52\n",
            "15/15 - 212s - loss: 0.1949 - accuracy: 0.9284 - 212s/epoch - 14s/step\n",
            "Epoch 52/52\n",
            "15/15 - 224s - loss: 0.2561 - accuracy: 0.9083 - 224s/epoch - 15s/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5055309e-801b-4c44-af10-b8ce0367df29\", \"model2.h5\", 27848176)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "model = load_model('model2.h5')\n",
        "model.summary()\n",
        "\n",
        "# predict = model.predict(validation_generator, steps=5)\n",
        "# print(validation_generator.class_indices)\n",
        "# print(\"예측값: \", predict)\n",
        "######################################################\n",
        "# TEST_DIR = \"/content/drive/MyDrive/dataset/test/\"\n",
        "# test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "# test_generator = test_datagen.flow_from_directory(\n",
        "#     TEST_DIR,\n",
        "#     target_size = (150,150),\n",
        "#     class_mode='categorical',\n",
        "# )\n",
        "\n",
        "img = image.load_img(\"/content/drive/MyDrive/dataset/test/KakaoTalk_20220315_150133955.jpg\", target_size=(150,150))\n",
        "img2 = image.img_to_array(img)\n",
        "img2 = np.expand_dims(img2, axis = 0)\n",
        "\n",
        "predict1 = model.predict(img2)\n",
        "# print(test_generator.class_indices)\n",
        "print(\"예측값1: \", predict1)\n",
        "\n",
        "if predict1[0][0]==1: print('heart')\n",
        "elif predict1[0][1]==1: print('hi')\n",
        "elif predict1[0][2]==1: print('pet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHCdILeCHCvQ",
        "outputId": "fd38650e-f0b7-4952-a3c3-1b6ac60131b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               3211776   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,473,475\n",
            "Trainable params: 3,473,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa5af828290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "예측값1:  [[0. 1. 0.]]\n",
            "hi\n"
          ]
        }
      ]
    }
  ]
}